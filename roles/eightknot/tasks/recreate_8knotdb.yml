- name: test aws credentials
  ansible.builtin.include_role:
    name: aws
    tasks_from: check_saml

- name: gather AWS EC2 metadata facts
  amazon.aws.ec2_metadata_facts:

- name: Gather snapshot info from AWS
  amazon.aws.ec2_snapshot_info:
    filters:
      owner-id: "{{ aws_account_id }}"
      "tag:createdby": ansible
    profile: "{{ aws_auth_profile }}"
    region: "{{ aws_region }}"
  register: snapshot_lookup_results
  delegate_to: localhost

- name: Determine the most recent snapshot created by this script
  set_fact:
    latest_snapshot: "{{ snapshot_lookup_results.snapshots | sort(attribute='start_time', reverse=true) | first }}"

- name: Fail if progress is not 100% on the snapshot
  ansible.builtin.fail:
    msg: >
      Snapshot creation progress for snapshot {{ latest_snapshot.snapshot_id }} is not at 100%.
      Actual progress: {{ latest_snapshot.progress }}.
  when: latest_snapshot.progress != "100%"

- name: Create new volume from snapshot {{ latest_snapshot.snapshot_id }}
  # Creates a new EBS volume based on the snapshot.
  # The size can be increased here if `new_volume_size_gb` is set.
  amazon.aws.ec2_vol:
    snapshot: "{{ latest_snapshot.snapshot_id }}"
    region: "{{ aws_region }}"
    zone: "{{ aws_zone }}"
    volume_type: "{{ new_volume_type }}"
    # size: "{{ new_volume_size_gb if new_volume_size_gb > 0 else omit }}" # Omit size if 0 to keep original
    profile: "{{ aws_auth_profile }}"
    tags:
      Name: "{{ new_pg_container_name }}-data-volume"
      Project: "PostgresMigration"
    state: present
  register: new_volume_result # Stores the new volume details
  delegate_to: localhost

- name: Bring 8knot down
  include_tasks: down.yml

# - name: unmount the augur db
#   include_tasks: unmount_augur.yml

- name: unmount the augur db
  block:
  - name: "Determine block device for {{ mount_point }}"
    ansible.builtin.set_fact:
      mount_point_block: "{{ ansible_mounts | json_query(query) | first }}"
    vars:
      query: "[?mount=='{{ mount_point }}'].device"
    when: "{{ mount_point is mount }}"

  # this should overwrite the fallback value in the variables if we can determine 
  # the actual partition device for the mount point
  - name: Set source partition values.
    ansible.builtin.set_fact:
      source_device_partition: "{{ mount_point_block }}"
    when: "{{ mount_point is mount }}"

  - name: Identify aws volume mounted at location
    identify_volume:
      partition_device: "{{ mount_point_block }}"
    register: aws_vol
    when: "{{ mount_point is mount }}"

  - name: unmount DB from {{ mount_point }}
    ansible.posix.mount:
      path: "{{ mount_point }}"
      src: "{{ mount_point_block }}"
      fstype: xfs
      state: unmounted
    become: true
    when: "{{ mount_point is mount }}"

- name: Attach new volume {{ new_volume_result.volume_id }} to instance {{ ansible_ec2_instance_id }} as {{ mount_point_block }}
  # Attaches the newly created volume to the target EC2 instance.
  amazon.aws.ec2_vol:
    instance: "{{ ansible_ec2_instance_id }}"
    id: "{{ new_volume_result.volume_id }}"
    device_name: "{{ mount_point_block }}" # Use the same device name or a new one
    region: "{{ aws_region }}"
    profile: "{{ aws_auth_profile }}"
    state: present
  register: attach_volume_result
  delegate_to: localhost

- name: Wait for the new device to be available on the host
  # Waits until the kernel recognizes the newly attached device.
  ansible.builtin.wait_for:
    path: "{{ mount_point_block }}" # This path needs to be the kernel's device path (e.g., /dev/xvdf)
    timeout: 60
    msg: "New device {{ mount_point_block }} did not appear within 60 seconds."

- name: remount the augur db
  include_tasks: mount_augur.yml

# TODO: verify the new mount point matches the new volume ID

- name: Bring 8knot up
  include_tasks: up.yml

# disable FSR?



# - name: Unmount and delete old volume ({{ source_volume_id }})
#   # CAUTION: This step permanently deletes your old volume.
#   # ONLY run this AFTER you have thoroughly verified that the new PostgreSQL
#   # instance is fully functional and data is correct.
#   # Consider commenting this out or running it manually after verification.
#   amazon.aws.ec2_vol:
#     id: "{{ source_volume_id }}"
#     region: "{{ aws_region }}"
#     state: absent
#   delegate_to: localhost
#   #only delete if the new container was started and the PostgreSQL verification command succeeded.
#   when: new_pg_container_start.changed and pg_check_result.rc == 0

# - name: Delete the snapshot ({{ snapshot_result.snapshot_id }})
#   # CAUTION: This step permanently deletes the snapshot.
#   # Keep the snapshot as a recovery point until you are absolutely certain
#   # the new setup is stable and you no longer need it.
#   amazon.aws.ec2_snapshot:
#     snapshot_id: "{{ snapshot_result.snapshot_id }}"
#     region: "{{ aws_region }}"
#     state: absent
#   delegate_to: localhost
#   # only delete if the new container was started and the PostgreSQL verification command succeeded.
#   when: new_pg_container_start.changed and pg_check_result.rc == 0



# - name: Delete the snapshot ({{ new_volume_result.snapshot_id }}) after volume creation
#   # This task removes the EBS snapshot immediately after a new volume has been successfully
#   # created from it. It relies on the 'new_volume_result' variable which should be
#   # registered from the 'Create new volume from snapshot' task.
#   #
#   # CAUTION: Deleting snapshots means you lose that specific point-in-time recovery point.
#   # Ensure this is the desired behavior for your workflow.
#   amazon.aws.ec2_snapshot:
#     snapshot_id: "{{ new_volume_result.snapshot_id }}"
#     region: "{{ aws_region }}"
#     state: absent
#   delegate_to: localhost
#   # ensures this task only runs if the 'new_volume_result'
#   # variable (from the volume creation task) contains a valid snapshot ID.
#   when: new_volume_result.snapshot_id is defined and new_volume_result.snapshot_id != ''