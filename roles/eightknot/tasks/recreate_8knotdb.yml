- name: test aws credentials
  ansible.builtin.include_role:
    name: aws
    tasks_from: check_saml

- name: gather AWS EC2 metadata facts
  amazon.aws.ec2_metadata_facts:

- name: Gather snapshot info from AWS
  amazon.aws.ec2_snapshot_info:
    filters:
      owner-id: "{{ aws_account_id }}"
      # "tag:createdby": ansible
      "tag:Project": PostgresMigration
    profile: "{{ aws_auth_profile }}"
    region: "{{ aws_region }}"
  register: snapshot_lookup_results
  delegate_to: localhost

- name: Determine the most recent snapshot created for this project
  set_fact:
    latest_snapshot: "{{ snapshot_lookup_results.snapshots | sort(attribute='start_time', reverse=true) | first }}"

- name: Fail if progress is not 100% on the snapshot
  ansible.builtin.fail:
    msg: >
      Snapshot creation progress for snapshot {{ latest_snapshot.snapshot_id }} is not at 100%.
      Actual progress: {{ latest_snapshot.progress }}.
  when: latest_snapshot.progress != "100%"

- name: Gather volume info from AWS
  amazon.aws.ec2_vol_info:
    filters:
      # owner id is implied - i dont think volumes can be public like snapshots
      # "tag:createdby": ansible
      "tag:Project": PostgresMigration
    profile: "{{ aws_auth_profile }}"
    region: "{{ aws_region }}"
  register: volume_lookup_results
  delegate_to: localhost

- name: Determine the most recent volume created for this project
  set_fact:
    latest_volume: "{{ volume_lookup_results.volumes | sort(attribute='create_time', reverse=true) | first }}"

- name: Create new volume from snapshot {{ latest_snapshot.snapshot_id }} (if there isnt a relatively new one available)
  # Creates a new EBS volume based on the snapshot.
  # The size can be increased here if `new_volume_size_gb` is set.
  amazon.aws.ec2_vol:
    snapshot: "{{ latest_snapshot.snapshot_id }}"
    region: "{{ aws_region }}"
    zone: "{{ aws_zone }}"
    volume_type: "{{ new_volume_type }}"
    # size: "{{ new_volume_size_gb if new_volume_size_gb > 0 else omit }}" # Omit size if 0 to keep original
    profile: "{{ aws_auth_profile }}"
    tags:
      Name: "{{ new_pg_container_name }}-data-volume"
      createdby: ansible
      Project: PostgresMigration
    state: present
  register: new_volume
  delegate_to: localhost
  when: (now(utc=True) - (latest_volume.create_time | to_datetime(iso8601format) )).days > 3
  vars:
    iso8601format: '%Y-%m-%dT%H:%M:%S.%f+00:00'
  
- name: use newly created volume as latest
  ansible.builtin.set_fact:
    latest_volume: "{{ new_volume }}"
  when: (now(utc=True) - (latest_volume.create_time | to_datetime(iso8601format) )).days > 3
  vars:
    iso8601format: '%Y-%m-%dT%H:%M:%S.%f+00:00'

- name: Bring 8knot down
  include_tasks: down.yml

- name: "Determine block device for {{ mount_point }}"
  block:
    - name: list matching mounts
      ansible.builtin.set_fact:
        mount_point_devices: "{{ ansible_mounts | json_query(query) }}"
      vars:
        query: "[?mount=='{{ mount_point }}'].device"

    - name: "Determine block device for {{ mount_point }}"
      ansible.builtin.set_fact:
        mount_point_block: "{{ mount_point_devices | first }}"
      when: mount_point_devices is defined and (mount_point_devices | length > 0)

    - name: "Determine block device for {{ mount_point }}"
      ansible.builtin.set_fact:
        mount_point_block: "{{ source_device_disk }}"
      when: mount_point_devices is not defined or (mount_point_devices | length == 0)

- name: Identify aws volume mounted at location
  identify_volume:
    partition_device: "{{ mount_point_block }}"
  register: aws_vol

- name: unmount DB from {{ mount_point }}
  ansible.posix.mount:
    path: "{{ mount_point }}"
    state: unmounted
  become: true

- name: Detach old volume {{ aws_vol.volume_id }} from instance {{ ansible_ec2_instance_id }} mounted at {{ mount_point_block }}
  amazon.aws.ec2_vol:
    id: "{{ aws_vol.volume_id }}"
    region: "{{ aws_region }}"
    profile: "{{ aws_auth_profile }}"
    state: absent
  register: detach_volume_result
  delegate_to: localhost

- name: Attach new volume {{ latest_volume.volume_id }} to instance {{ ansible_ec2_instance_id }} as {{ mount_point_block }}
  # Attaches the newly created volume to the target EC2 instance.
  amazon.aws.ec2_vol:
    instance: "{{ ansible_ec2_instance_id }}"
    id: "{{ latest_volume.volume_id }}"
    device_name: "{{ mount_point_block }}" # Use the same device name or a new one
    region: "{{ aws_region }}"
    profile: "{{ aws_auth_profile }}"
    state: present
  register: attach_volume_result
  delegate_to: localhost

- name: Wait for the new device to be available on the host
  # Waits until the kernel recognizes the newly attached device.
  ansible.builtin.wait_for:
    path: "{{ mount_point_block }}" # This path needs to be the kernel's device path (e.g., /dev/xvdf)
    timeout: 60
    msg: "New device {{ mount_point_block }} did not appear within 60 seconds."

- name: remount the augur db
  include_tasks: mount_augur.yml

# TODO: verify the new mount point matches the new volume ID

- name: Bring 8knot up
  include_tasks: up.yml

# disable FSR?



# - name: Unmount and delete old volume ({{ aws_vol.volume_id }})
#   # CAUTION: This step permanently deletes your old volume.
#   # ONLY run this AFTER you have thoroughly verified that the new PostgreSQL
#   # instance is fully functional and data is correct.
#   # Consider commenting this out or running it manually after verification.
#   amazon.aws.ec2_vol:
#     id: "{{ aws_vol.volume_id }}"
#     region: "{{ aws_region }}"
#     state: absent
#   delegate_to: localhost
#   #only delete if the new container was started and the PostgreSQL verification command succeeded.
#   when: new_pg_container_start.changed and pg_check_result.rc == 0

# - name: Delete the snapshot ({{ snapshot_result.snapshot_id }})
#   # CAUTION: This step permanently deletes the snapshot.
#   # Keep the snapshot as a recovery point until you are absolutely certain
#   # the new setup is stable and you no longer need it.
#   amazon.aws.ec2_snapshot:
#     snapshot_id: "{{ snapshot_result.snapshot_id }}"
#     region: "{{ aws_region }}"
#     state: absent
#   delegate_to: localhost
#   # only delete if the new container was started and the PostgreSQL verification command succeeded.
#   when: new_pg_container_start.changed and pg_check_result.rc == 0



# - name: Delete the snapshot ({{ latest_volume.snapshot_id }}) after volume creation
#   # This task removes the EBS snapshot immediately after a new volume has been successfully
#   # created from it. It relies on the 'latest_volume' variable which should be
#   # registered from the 'Create new volume from snapshot' task.
#   #
#   # CAUTION: Deleting snapshots means you lose that specific point-in-time recovery point.
#   # Ensure this is the desired behavior for your workflow.
#   amazon.aws.ec2_snapshot:
#     snapshot_id: "{{ latest_volume.snapshot_id }}"
#     region: "{{ aws_region }}"
#     state: absent
#   delegate_to: localhost
#   # ensures this task only runs if the 'latest_volume'
#   # variable (from the volume creation task) contains a valid snapshot ID.
#   when: latest_volume.snapshot_id is defined and latest_volume.snapshot_id != ''